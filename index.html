<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Self-Supervised Learning Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      background: #f6f7fb;
      color: #111827;
    }

    .page {
      max-width: 960px;
      margin: 0 auto;
      padding: 24px 16px 48px;
    }

    header {
      margin-bottom: 24px;
      border-bottom: 1px solid #e5e7eb;
      padding-bottom: 12px;
    }

    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }

    header p {
      margin: 4px 0 0;
      color: #4b5563;
      font-size: 0.95rem;
    }

    h2 {
      margin-top: 32px;
      margin-bottom: 12px;
      font-size: 1.3rem;
      border-bottom: 1px solid #e5e7eb;
      padding-bottom: 4px;
    }

    .paper-list {
      display: flex;
      flex-direction: column;
      gap: 16px;
      margin-top: 8px;
    }

    .paper-card {
      background: #ffffff;
      border-radius: 8px;
      padding: 16px 18px;
      box-shadow: 0 1px 3px rgba(15, 23, 42, 0.08);
    }

    .paper-title {
      font-weight: 600;
      margin-bottom: 4px;
    }

    .paper-meta {
      font-size: 0.88rem;
      color: #6b7280;
      margin-bottom: 8px;
    }

    .paper-meta span + span::before {
      content: "•";
      margin: 0 6px;
    }

    .paper-link a {
      font-size: 0.88rem;
      text-decoration: none;
      color: #2563eb;
    }

    .paper-link a:hover {
      text-decoration: underline;
    }
    .paper-figure {
      margin-top: 12px;
      text-align: center;
    }
    
    /* .paper-figure img {
      max-width: 100%;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.15);
    } */
    .paper-figure img {
      width: 60%;       /* 40–70% usually looks nice */
      height: auto;
    }
    .paper-figure .caption {
      font-size: 0.8rem;
      color: #6b7280;
      margin-top: 6px;
    }
    .paper-summary p {
      margin: 6px 0;
      font-size: 0.92rem;
      line-height: 1.5;
    }

    /* Table section */
    .table-wrapper {
      margin-top: 16px;
      overflow-x: auto;
      background: #ffffff;
      padding: 16px;
      border-radius: 8px;
      box-shadow: 0 1px 3px rgba(15, 23, 42, 0.08);
    }

    table {
      border-collapse: collapse;
      width: 100%;
      font-size: 0.9rem;
    }

    th, td {
      border: 1px solid #e5e7eb;
      padding: 6px 8px;
      text-align: center;
      vertical-align: middle;
    }

    th {
      background: #f3f4f6;
      font-weight: 600;
    }

    td:first-child,
    th:first-child {
      text-align: left;
      white-space: nowrap;
    }

    .check {
      font-size: 1rem;
    }

    .check::before {
      content: "✓";
      color: #16a34a;
      font-weight: 600;
    }

    .empty {
      color: #d1d5db;
    }

    footer {
      margin-top: 32px;
      font-size: 0.8rem;
      color: #9ca3af;
      text-align: center;
    }

    @media (max-width: 600px) {
      header h1 {
        font-size: 1.4rem;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header>
      <h1>Self-Supervised Learning Paper Notes</h1>
      <p>Summaries and feature comparison for different types of SSL algorithms.</p>
    </header>

    <!-- PAPERS SECTION -->
    <section>
      <h2>Papers</h2>
      <div class="paper-list">

        <!-- Example paper card: edit / duplicate this block for more papers -->
        <article class="paper-card">
          <div class="paper-title">
            [1] Masked Autoencoders Are Scalable Vision Learners
          </div>
          <div class="paper-meta">
            <span>CVPR 2022</span>
            <span>Citations: 12,652 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Autoencoding has been widely used in NLP for self-supervision. However, when applying this technique to computer vision, 
              there are two key challenges. (1) In NLP, the words in a sentence are masked and the task is to predict them. The missing words 
              contain rich semantic information, however that is not the same case with vision, because pixels carry a lower semantic level of information. 
              Masking a few pixels can be easily recovered through the help of extrapolation from the neighboring pixels. (2) Previous denoising autoencoding (DAE) 
              methods in vision process the entire image (containing both masked and non-masked pixels as the input to the encoder). With convolutional networks, 
              it was not possible to integrate the masked tokens or positional embeddings in the latent space. However, with the help of ViT, it can be easily 
              overcome (making training 3x faster). Additionally, previous DAE calculated loss on all pixels rather than just the masked pixels, which proved to 
              be slightly bad (just an empirical evidence). 
            </p>
            <p>
              The masked autoencoder (MAE) [1] consists of three steps. First, randomly masking a very high percentage of patches of the image, because masking 
              a high proportion of the input image yields a non-trivial and meaningful self-supervisory task. Only the unmasked patches are passed through the 
              encoder (<25%), reducing compute and memory, and enabling the designing of very large encoders. The latent representations of the unmasked patches 
                are concatenated with the learnable mask tokens and are then passed into the decoder. The decoder reconstructs the pixels of the image. The design 
                of decoder plays an important role in determining the semantic level of learned latent representations. A decoder with deeper depth enables the 
                encoder to learn useful representations because the last several layers of the decoder can account for the reconstruction specializing, leaving 
                the latent representations at a more abstract level. Conversely, if the decoder is shallower, the encoder is now forced to aid the decoder with 
                reconstructing the pixels, rather than learning good representations. 
            </p>
            <div class="paper-figure">
              <img src="images/mae.png" alt="MAE architecture">
              <p class="caption">Figure: MAE Architecture [1].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [2] A Simple Framework for Contrastive Learning of Visual Representations
          </div>
          <div class="paper-meta">
            <span>ICML 2020</span>
            <span>Citations: 28,187 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.mlr.press/v119/chen20j/chen20j.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Contrastive learning of visual representations overcomes the limitation of representation collapse, however, the prior works have three key 
              limitations. (1) Designing specialized architectures for creating effective predictive tasks, which increases algorithm complexity. (2) Using 
              memory banks to store embeddings for negative sampling, where many stored embeddings were computed by older versions of the encoder and no longer 
              match the current model’s representations, leading to inconsistent negatives and added training overhead.  (3) Leveraging handcrafted pretext tasks 
              (e.g. rotation prediction, jigsaw, etc.), which risked the network exploiting heuristics to minimize loss rather than learning generalizable 
              semantic representations.
            </p>
            <p>
              The framework termed SimCLR [2] involves an integration of three components. Initially, two transformations of the same image are produced using 
              (1) a series of stochastic data augmentations, achieving two correlated views. These two views are then passed through a base encoder network and 
              (2) a projection head and trained to maximize agreement using the (3) contrastive loss, which pulls the representations of these two views closer, 
              while pushing the dissimilar representations of the views of two different image-pairs (negative samples) away. After training is complete, the 
              projection head is thrown away and the encoder is used as a learned representation for downstream tasks. (1) Data augmentation (e.g. cropping+color 
              jitter) defines an effective discriminative task, instead of relying on specialized architectures. The composition of using multiple augmentations 
              is necessary because cropped patches of the same image still share similar color histograms, and the neural network may be using this shortcut to 
              minimize loss rather than learning meaningful representations. (2) Adding a learnable nonlinear transformation between the encoder representations 
              and contrastive loss allows the encoder to learn better representations. (3) The Normalized Temperature-scaled Cross-Entropy (NT-Xent) contrastive 
              loss uses cosine similarity, which normalizes feature magnitudes and focuses learning on the angular relationship between representations, leading 
              to more stable and effective contrastive updates. Contrastive learning benefits from larger batch sizes and longer training because more in-batch 
              negatives provide a stronger contrastive signal.
            </p>
            <div class="paper-figure">
              <img src="images/simclr.png" alt="SimCLR architecture">
              <p class="caption">Figure: SimCLR Architecture [2].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [3] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
          </div>
          <div class="paper-meta">
            <span>NeurIPS 2020</span>
            <span>Citations: 9,592 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              For self-supervised learning using a cross-view regression framework, the representation of the two correlated views are aligned, but leads to 
              representation collapse, i.e. the network maps all inputs to the same constant representation. To overcome this, contrastive learning (CL) methods 
              [2] rely on discriminating positive pairs from many negative-pair examples, which makes them highly dependent on large batch sizes. Also, they 
              require multiple stochastic data augmentations for learning good representations. 
            </p>
            <p>
              The BYOL [3] algorithm addresses the above challenges posed by CL. It begins by providing two augmented views of the same image as input to a 
              student and a teacher network. The teacher consists of an encoder and a projection head, while the student consists of the same but also includes 
              a predictor function, which predicts the representations of the teacher. The loss between the representations of teacher and student predictions are 
              minimized. When we use a randomly initialized, fixed network to produce teacher representations and train the student to predict those 
              representations, it avoids collapse, though the resulting representations are weak. Subsequently, if a student can improve upon a random fixed 
              teacher, then iteratively predicting better teacher representations yields increasingly stronger representations. As a result, from the teacher 
              representation, a potentially improved representation using the student network can be trained by predicting that teacher representation. Repeating 
              this process creates a sequence of progressively refined representations, using each student network as the next teacher. BYOL generalizes this 
              bootstrapping idea by updating the weights of only the student network and using a slow exponential moving average of the student network as the 
              teacher network.
            </p>
            <div class="paper-figure">
              <img src="images/byol.png" alt="BYOL architecture">
              <p class="caption">Figure: BYOL Architecture [3].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [4] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
          </div>
          <div class="paper-meta">
            <span>CVPR 2023</span>
            <span>Citations: 748 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Discriminative methods [2,3] may introduce strong biases that affect certain downstream tasks or pretraining tasks with different data distributions. 
              On the other hand, predictive methods like MAE [1] provide representations of lower semantic value relative to the discriminative methods and thus 
              require end-to-end fine-tuning to perform well on the downstream tasks. To address these limitations, I-JEPA aims to learn semantic representations 
              that do not require extensive finetuning on downstream tasks, and unlike the discriminative methods, do not incorporate extra prior knowledge 
              encoded through image transformations.
            </p>
            <p>
              I-JEPA learns abstract representations through the prediction of other regions of a masked image in the latent space. It consists of three main 
              components; context, target encoders and a predictor. The context encoder processes the visible context patches, while the predictor takes the 
              context encoder output and, conditioned on learnable mask tokens with added positional embeddings of the target block, predicts the target 
              representations at a specific location. It does this for various target blocks. The target representations are the outputs of the target-encoder. 
              The L2 loss is minimized between each block representation of the target encoder and the block predictions made by the predictor. The weights of 
              context encoder and predictor are updated using gradient descent while the weights of the target encoder are updated through the exponential moving 
              average (EMA) of the context encoder weights to avoid representation collapse. Similar to MAE [1], the masking strategy plays an important role to 
              learn meaningful representations. As a result, the target blocks need to be large enough to contain semantic information while the context block 
              should be spatially informative.  
            </p>
            <div class="paper-figure">
              <img src="images/i-jepa.png" alt="I-JEPA architecture">
              <p class="caption">Figure: I-JEPA Architecture [4].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [5] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language
          </div>
          <div class="paper-meta">
            <span>ICML 2022</span>
            <span>Citations: 1,205 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.mlr.press/v162/baevski22a/baevski22a.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Most prior research in self-supervised learning has focused on a specific modality (e.g. image, words, or speech), which leads to designing specific 
              architectures and learning biases that may not generalize to other modalities. Moreover, in computer vision, prior works consist of regressing/predicting 
              targets that contain information isolated to the current patch, such as visual tokens or pixels, which lack contextualized information (relevant features 
              from the entire image).  While some discriminative methods [3] use the latent target representation of the entire input, their aim is to learn 
              transformation-invariant features instead of structural information within a sample.
            </p>
            <p>
              Data2vec proposes regressing over the latent representations of the full input data using a masked view of the same input. It primarily consists of 
              a student and a teacher mode which share the same architecture. The teacher consists of taking as input a sequence of encoded tokens of the full 
              input and providing it to the ViT to get the latent representations. The student consists of transforming the same input into encoded tokens and 
              then randomly replacing tokens with a learnable mask embedding token, and feeding it to the ViT. The L2 loss between only the masked tokens of the 
              student and the corresponding positional tokens in the teacher representations are regressed. The weights of the student are updated using gradient 
              descent while the weights of the teacher are updated using the EMA of the student network. The teacher provides contextualized representations, 
              which contain richer information of the input data due to the self-attention mechanism of the transformer network. Moreover, they are normalized 
              and averaged outputs of last k-layers of the teacher network, since neural networks build features over multiple layers and providing different 
              types of features from multiple layers enriches the self-supervisory task.   
            </p>
            <div class="paper-figure">
              <img src="images/data2vec.png" alt="data2vec architecture">
              <p class="caption">Figure: Data2vec Architecture [5].</p>
            </div>
          </div>
        </article>
        
        <article class="paper-card">
          <div class="paper-title">
            [6] iBOT: Image Bert pre-training with Online Tokenizer
          </div>
          <div class="paper-meta">
            <span>ICLR 2022</span>
            <span>Citations: 1,310 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openreview.net/pdf?id=ydopy-e6Dg" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior works have focused primarily on the global views [2,3] for self-supervised learning, neglecting the internal structures of an image. 
              Empirically, the visual semantics tend to emerge progressively by bootstrapping the teacher representation that enforces similarity across 
              augmented views, but this process alone only captures global information and ignores the meaningful local patterns inside an image. As a result, 
              existing methods struggle to learn meaningful visual/patch-level semantics.
            </p>
            <p>
              iBOT combines learning local structural information using masked image modeling and global visual semantics using the cross-view alignment. In iBOT, 
              an image is sampled and two augmented views are created. Both views are passed through a student and a teacher network, but the view that's input to 
              the student undergoes random masking. The student encodes these masked patches using a ViT, while the teacher encodes the unmasked view. The 
              student’s masked patch tokens are aligned to the corresponding teacher tokens using a cross-entropy (CE) loss. At the same time, iBOT also aligns 
              the [CLS] tokens across the two views, using CE loss, so that the model learns global visual semantics and transformation-invariant features between 
              the two correlated (augmented) views. The student is updated by gradient descent, and the teacher is updated as an EMA of the student. Moreover, the 
              same projection head is shared by the [CLS] token and each patch token to borrow the capability of semantic abstraction acquired from the [CLS] token. 
            </p>
            <div class="paper-figure">
              <img src="images/ibot.png" alt="ibot architecture">
              <p class="caption">Figure: iBOT Architecture [6].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [7] Emerging properties in self-supervised vision transformers 
          </div>
          <div class="paper-meta">
            <span>ICCV 2021</span>
            <span>Citations: 9,350 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              ViTs have shown competitive performance against convolutional networks, however, they haven’t shown clear benefits over them in a supervised 
              learning setup. They require more computation, large amounts of training data to generalize and their features do not exhibit any unique properties. 
              Moreover, supervised learning does not leverage the context rich features present in an image and may use only a single concept from a predefined set 
              of a few thousand categories of objects.  Prior methods have proposed contrastive loss, adding predictor, or advanced normalization to avoid 
              representation collapse, however they add little benefit to stability of training and performance. Perhaps a simpler approach could be leveraged 
              while enjoying high performance. 
            </p>
            <p>
              DINO demonstrates that self-supervised ViT features contain rich semantics and perform very well with a k-nearest neighbor classifier on top, without 
              requiring finetuning. DINO begins by sampling an image and creating a set of augmented views that include two global views (>50% of the image) and 
              several local views (<50%). The global views are input to a teacher network, and the local views to a student network. Each network outputs a 
              K-dimensional feature that is normalized using a temperature-scaled softmax over the feature dimension. The latent representations of both networks 
              are aligned using a cross-entropy loss, encouraging “local-to-global’’ correspondences. The student is updated using SGD, while the teacher is 
              updated as an EMA of the student. DINO does not use batch-normalization and demonstrates that using smaller ViT patch sizes improves feature quality. 
              Representation collapse in this setting is not only about predicting constant vectors, but also includes uniform distributions (each feature ≈ 1/K) or 
              overly peaked distributions dominated by one dimension. Instead of using a predictor as in [3], DINO avoids collapse through centering and sharpening of 
              teacher representations. They are first centered by subtracting their running average (updated via EMA), which stabilizes training and enables 
              robustness across batch sizes. The centered outputs are then sharpened by dividing by a very small temperature and applying softmax. Centering alone 
              risks drifting toward uniform outputs, while sharpening alone risks producing peaked distributions, so applying both together achieves the best of both 
              worlds, avoiding collapse.
            </p>
            <div class="paper-figure">
              <img src="images/dino.png" alt="dino architecture">
              <p class="caption">Figure: DINO Architecture [7].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [8] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments 
          </div>
          <div class="paper-meta">
            <span>NeurIPS 2020</span>
            <span>Citations: 5,478 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Contrastive learning (CL) shows that aligning features from two augmented views while pushing apart negatives helps avoid collapse and yields good 
              representations. However, CL relies on random subsets of images to approximate-the-loss since it cannot use each and every pair to train, and also 
              needs very large batch sizes to generalize. Prior clustering-based approaches try to overcome this by assigning each image to a prototype and 
              training the model to predict these assignments for augmented views. Yet these methods follow a two-step offline approach where cluster assignments 
              must be recomputed over the entire dataset, which becomes impractical as dataset size grows. Another limitation is that, despite evidence that 
              multiple augmentations improve invariance, most methods still operate with only two augmented views.
            </p>
            <p>
              SwAV proposes to approximate-the-task by incorporating online clustering. For every image, several augmented views are created and passed through 
              an encoder to get feature vectors. Instead of directly comparing these features like in [2], each feature is assigned to one of several learnable 
              prototype vectors, which act like cluster centers. These assignments are called “codes.” Inspired by CL, SwAV then uses swapped alignment: the 
              feature from one view must align with the code produced by another view of the same image, and vice-versa. If the two views contain the same 
              underlying content, their codes should match, so this cross-view alignment encourages the encoder to learn consistent and meaningful representations. 
              To compute codes without storing the whole dataset, SwAV uses only the features within the current batch and applies a Sinkhorn normalization step 
              to ensure that prototypes are used evenly across the batch. This avoids trivial collapse where all features map to the same prototype. The method 
              also proposes using multi-crop augmentation, where one encoder receives two global views and the other encoder receives several local, low-resolution 
              crops of different sizes, to provide more diverse views and avoid biases in the features. Altogether, SwAV combines online clustering, balanced 
              assignments, and swapped alignment to learn representations without explicit negative samples or large memory banks.
            </p>
            <div class="paper-figure">
              <img src="images/swav.png" alt="swav architecture">
              <p class="caption">Figure: SwAV Architecture [8].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [9] Context Autoencoder for Self-supervised Representation Learning
          </div>
          <div class="paper-meta">
            <span>IJCV 2024</span>
            <span>Citations: 524 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://link.springer.com/content/pdf/10.1007/s11263-023-01852-4.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              The encoder of an SSL algorithm must learn highly informative and context-rich representations during pretraining to perform well on downstream 
              tasks. However, several prior works operating in the latent space do not explicitly separate representation learning from task completion, and ViTs 
              effectively mix encoder and decoder roles, which can limit representation capacity. For instance, in the masked image modeling component of the iBOT 
              algorithm [6], it does not separate the representation role and task completion role and uses both visible and masked patches as input, 
              simultaneously for the two roles. Similarly, in MAE [1], the decoder partially participates in representation learning because representations of 
              visible patches are also updated within the decoder, dispersing semantic learning across encoder-decoder. In view-invariance based methods [2,3], 
              cropping is critical for representation learning, yet most cropping strategies consistently include the image center. Since ImageNet-1K images 
              predominantly contain centered objects, this biases the model toward learning representations mainly from central regions, limiting coverage of the 
              full image. Consequently, objects away from the center may be treated as noise during contrastive learning, reducing the diversity of learned 
              representations.
            </p>
            <p>
              CAE proposes an encoder-regressor-decoder algorithm for representation learning. It decouples representation learning from solving the pretext task, 
              i.e., masked representation prediction and masked patch reconstruction. In the first step, an image is split into a sequence of patches, and only the 
              visible patches are passed through the encoder. In the second step, the latent representations of the visible patches, along with learnable masked 
              patch tokens, and their positional embeddings are passed into a regressor. The regressor consists of transformer blocks with cross-attention, where masked patch 
              tokens serve as queries and visible patch representations serve as keys and values, to predict the latent representations of the masked patches. In 
              the third step, the predicted masked latent representations are input to the decoder to reconstruct the discrete visual tokens of the masked patches, 
              i.e., tokens produced by a d-VAE trained on ImageNet-1K or the DALL-E tokenizer. The predicted masked latent representations are aligned with the masked 
              representations obtained from a target encoder using MSE loss. The gradients are stopped on the target encoder branch to prevent collapse. Simultaneously, the patch-level 
              reconstruction is optimized using cross-entropy loss over the discrete visual tokens. The encoder operates only on the visible patches, and CAE enables it to learn 
              context-rich representations of all the features present in an image.
            </p>
            <div class="paper-figure">
              <img src="images/cae.png" alt="cae architecture">
              <p class="caption">Figure: CAE Architecture [9].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [10] SimMIM: a Simple Framework for Masked Image Modeling
          </div>
          <div class="paper-meta">
            <span>CVPR 2022</span>
            <span>Citations: 1,981 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Masked signal modeling has shown great success with NLP in the past, and it has also widely been adapted in the computer vision area. When predicting 
              masked words, a self-supervised model learns features of high semantic value, however in vision, pixels carry low-level semantics, and the 
              reconstruction task becomes trivial if only a low percentage of pixels are masked in an image [1]. Recent methods have addressed this, but they use 
              specialized designs such as converting continuous signals into clusters, patch tokenization using an additional network, or block-wise masking 
              strategy to break short-range connections.
            </p>
            <p>
              SimMIM proposes a very simple framework that can be used for downstream tasks by finetuning the model once it has been pretrained. It consists of 
              three steps. (1) Randomly masking the patches within an image either using large patch sizes (e.g. 32) or high masking ratios (>80%), which 
              significantly reduces the chance of extrapolating neighboring pixels to recover the pixels. (2) The masked patches are replaced with learnable mask 
              token vectors, and then along with the visible patches, they are input to a ViT for obtaining the latent representations. (3) The latent 
              representations are decoded using only a single linear layer and the L1 loss is applied between only the original and predicted masked pixels. 
              SimMIMs main goal is to learn representations which can complement the following down-stream tasks by finetuning, rather than focussing on performing 
              well through linear probing.  
            </p>
            <div class="paper-figure">
              <img src="images/simmim.png" alt="simmim architecture">
              <p class="caption">Figure: SimMIM Architecture [10].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [11] Masked Siamese Networks for Label-Efficient Learning
          </div>
          <div class="paper-meta">
            <span>ECCV 2022</span>
            <span>Citations: 468 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910442.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior works in masked image modeling [1] have shown great performance in self-supervised learning. However, optimizing the reconstruction loss 
              requires modeling low-level image details that are not necessary for the downstreams tasks that involve semantic abstraction. Moreover, they 
              require longer training hours and increased memory and computation to train and generalize.  
            </p>
            <p>
              MSN is a joint-embedding architecture. It learns representations through both view-transformations and masked image modeling. In the first step, 
              two instances of the same image are created and undergo random data augmentations. Let one be referred to as the anchor view and other as target view. 
              The anchor view then undergoes masking (either random or focal), after which it is passed to ViT encoder to obtain the [CLS] token output as its latent representation. 
              Similarly, the target view is also passed through a ViT to obtain its [CLS] token. A soft-distribution over a set of prototypes for both the anchor 
              and target views are obtained, similar to the online clustering approach [8]. The cross entropy loss is minimized, where the objective is to assign 
              the representation of the masked anchor view to the same prototypes as the representation of the unmasked target view. Additionally, a mean entropy 
              maximization regularizer is used to encourage the model to utilize the full set of prototypes. The weights of the anchor view are updated via 
              gradient descent while the weights of the target are updated through EMA of the anchor.
            </p>
            <div class="paper-figure">
              <img src="images/msn.png" alt="msn architecture">
              <p class="caption">Figure: MSN Architecture [11].</p>
            </div>
          </div>
        </article>
      </div>
    </section>

    <!-- TABLE SECTION -->
    <h2>Algorithm vs Feature Comparison</h2>
    <section>
      <h3>Input</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Random Masking</th>
              <th>Data Augmentation</th>
              <th>Multiple Augmentations</th>
              <th>Negative-pair samples</th>
              <th>Local views</th>
              <th>Global views</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Architecture</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Predictive method</th>
              <th>Alignment method</th>
              <th>View-invariance method</th>
              <th>Asymmetric Architecture</th>
              <th>Bootstrapping</th>
              <th>Normalization</th>
              <th>Clustering</th>
              <th>Last-k layer averaged target</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Loss</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>MAE/L1</th>
              <th>MSE/L2</th>
              <th>CE</th>
              <th>Contrastive Learning</th>
              <th>Patch-level</th>
              <th>Representation-level</th>
              <th>Any extra regularizer</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Inductive biases</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Prior knowledge</th>
              <th>Learns representations through view-transformations</th>
              <th>Learns representations through structural information</th>
              <th>Full image contextualization</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Training requirements</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Large batch sizes increase performance</th>
              <th>Long training (> 1500 epochs)</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    
    

    <!-- <footer>
      
    </footer> -->
  </div>
</body>
</html>
