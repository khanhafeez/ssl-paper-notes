<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Self-Supervised Learning Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      background: #f6f7fb;
      color: #111827;
    }

    .page {
      max-width: 960px;
      margin: 0 auto;
      padding: 24px 16px 48px;
    }

    header {
      margin-bottom: 24px;
      border-bottom: 1px solid #e5e7eb;
      padding-bottom: 12px;
    }

    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }

    header p {
      margin: 4px 0 0;
      color: #4b5563;
      font-size: 0.95rem;
    }

    h2 {
      margin-top: 32px;
      margin-bottom: 12px;
      font-size: 1.3rem;
      border-bottom: 1px solid #e5e7eb;
      padding-bottom: 4px;
    }

    .paper-list {
      display: flex;
      flex-direction: column;
      gap: 16px;
      margin-top: 8px;
    }

    .paper-card {
      background: #ffffff;
      border-radius: 8px;
      padding: 16px 18px;
      box-shadow: 0 1px 3px rgba(15, 23, 42, 0.08);
    }

    .paper-title {
      font-weight: 600;
      margin-bottom: 4px;
    }

    .paper-meta {
      font-size: 0.88rem;
      color: #6b7280;
      margin-bottom: 8px;
    }

    .paper-meta span + span::before {
      content: "•";
      margin: 0 6px;
    }

    .paper-link a {
      font-size: 0.88rem;
      text-decoration: none;
      color: #2563eb;
    }

    .paper-link a:hover {
      text-decoration: underline;
    }
    .paper-figure {
      margin-top: 12px;
      text-align: center;
    }
    
    /* .paper-figure img {
      max-width: 100%;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.15);
    } */
    .paper-figure img {
      width: 60%;       /* 40–70% usually looks nice */
      height: auto;
    }
    .paper-figure .caption {
      font-size: 0.8rem;
      color: #6b7280;
      margin-top: 6px;
    }
    .paper-summary p {
      margin: 6px 0;
      font-size: 0.92rem;
      line-height: 1.5;
    }

    /* Table section */
    .table-wrapper {
      margin-top: 16px;
      overflow-x: auto;
      background: #ffffff;
      padding: 16px;
      border-radius: 8px;
      box-shadow: 0 1px 3px rgba(15, 23, 42, 0.08);
    }

    table {
      border-collapse: collapse;
      width: 100%;
      font-size: 0.9rem;
    }

    th, td {
      border: 1px solid #e5e7eb;
      padding: 6px 8px;
      text-align: center;
      vertical-align: middle;
    }

    th {
      background: #f3f4f6;
      font-weight: 600;
    }

    td:first-child,
    th:first-child {
      text-align: left;
      white-space: nowrap;
    }

    .check {
      font-size: 1rem;
    }

    .check::before {
      content: "✓";
      color: #16a34a;
      font-weight: 600;
    }

    .empty {
      color: #d1d5db;
    }

    footer {
      margin-top: 32px;
      font-size: 0.8rem;
      color: #9ca3af;
      text-align: center;
    }

    @media (max-width: 600px) {
      header h1 {
        font-size: 1.4rem;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header>
      <h1>Self-Supervised Learning Paper Notes</h1>
      <p>Summaries and feature comparison for different types of SSL algorithms.</p>
    </header>

    <!-- PAPERS SECTION -->
    <section>
      <h2>Papers</h2>
      <div class="paper-list">

        <!-- Example paper card: edit / duplicate this block for more papers -->
        <article class="paper-card">
          <div class="paper-title">
            [1] Masked Autoencoders Are Scalable Vision Learners
          </div>
          <div class="paper-meta">
            <span>CVPR 2022</span>
            <span>Citations: 12,652 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Autoencoding has been widely used in NLP for self-supervision. However, when applying this technique to computer vision, 
              there are two key challenges. (1) In NLP, the words in a sentence are masked and the task is to predict them. The missing words 
              contain rich semantic information, however that is not the same case with vision, because pixels carry a lower semantic level of information. 
              Masking a few pixels can be easily recovered through the help of extrapolation from the neighboring pixels. (2) Previous denoising autoencoding (DAE) 
              methods in vision process the entire image (containing both masked and non-masked pixels as the input to the encoder). With convolutional networks, 
              it was not possible to integrate the masked tokens or positional embeddings in the latent space. However, with the help of ViT, it can be easily 
              overcome (making training 3x faster). Additionally, previous DAE calculated loss on all pixels rather than just the masked pixels, which proved to 
              be slightly bad (just an empirical evidence). 
            </p>
            <p>
              The masked autoencoder (MAE) [1] consists of three steps. First, randomly masking a very high percentage of patches of the image, because masking 
              a high proportion of the input image yields a non-trivial and meaningful self-supervisory task. Only the unmasked patches are passed through the 
              encoder (<25%), reducing compute and memory, and enabling the designing of very large encoders. The latent representations of the unmasked patches 
                are concatenated with the learnable mask tokens and are then passed into the decoder. The decoder reconstructs the pixels of the image. The design 
                of decoder plays an important role in determining the semantic level of learned latent representations. A decoder with deeper depth enables the 
                encoder to learn useful representations because the last several layers of the decoder can account for the reconstruction specializing, leaving 
                the latent representations at a more abstract level. Conversely, if the decoder is shallower, the encoder is now forced to aid the decoder with 
                reconstructing the pixels, rather than learning good representations. 
            </p>
            <div class="paper-figure">
              <img src="images/mae.png" alt="MAE architecture">
              <p class="caption">Figure: MAE Architecture [1].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [2] A Simple Framework for Contrastive Learning of Visual Representations
          </div>
          <div class="paper-meta">
            <span>ICML 2020</span>
            <span>Citations: 28,187 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.mlr.press/v119/chen20j/chen20j.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Contrastive learning of visual representations overcomes the limitation of representation collapse, however, the prior works have three key 
              limitations. (1) Designing specialized architectures for creating effective predictive tasks, which increases algorithm complexity. (2) Using 
              memory banks to store embeddings for negative sampling, where many stored embeddings were computed by older versions of the encoder and no longer 
              match the current model’s representations, leading to inconsistent negatives and added training overhead.  (3) Leveraging handcrafted pretext tasks 
              (e.g. rotation prediction, jigsaw, etc.), which risked the network exploiting heuristics to minimize loss rather than learning generalizable 
              semantic representations.
            </p>
            <p>
              The framework termed SimCLR [2] involves an integration of three components. Initially, two transformations of the same image are produced using 
              (1) a series of stochastic data augmentations, achieving two correlated views. These two views are then passed through a base encoder network and 
              (2) a projection head and trained to maximize agreement using the (3) contrastive loss, which pulls the representations of these two views closer, 
              while pushing the dissimilar representations of the views of two different image-pairs (negative samples) away. After training is complete, the 
              projection head is thrown away and the encoder is used as a learned representation for downstream tasks. (1) Data augmentation (e.g. cropping+color 
              jitter) defines an effective discriminative task, instead of relying on specialized architectures. The composition of using multiple augmentations 
              is necessary because cropped patches of the same image still share similar color histograms, and the neural network may be using this shortcut to 
              minimize loss rather than learning meaningful representations. (2) Adding a learnable nonlinear transformation between the encoder representations 
              and contrastive loss allows the encoder to learn better representations. (3) The Normalized Temperature-scaled Cross-Entropy (NT-Xent) contrastive 
              loss uses cosine similarity, which normalizes feature magnitudes and focuses learning on the angular relationship between representations, leading 
              to more stable and effective contrastive updates. Contrastive learning benefits from larger batch sizes and longer training because more in-batch 
              negatives provide a stronger contrastive signal.
            </p>
            <div class="paper-figure">
              <img src="images/simclr.png" alt="SimCLR architecture">
              <p class="caption">Figure: SimCLR Architecture [2].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [3] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
          </div>
          <div class="paper-meta">
            <span>NeurIPS 2020</span>
            <span>Citations: 9,592 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              For self-supervised learning using a cross-view regression framework, the representation of the two correlated views are aligned, but leads to 
              representation collapse, i.e. the network maps all inputs to the same constant representation. To overcome this, contrastive learning (CL) methods 
              [2] rely on discriminating positive pairs from many negative-pair examples, which makes them highly dependent on large batch sizes. Also, they 
              require multiple stochastic data augmentations for learning good representations. 
            </p>
            <p>
              The BYOL [3] algorithm addresses the above challenges posed by CL. It begins by providing two augmented views of the same image as input to a 
              student and a teacher network. The teacher consists of an encoder and a projection head, while the student consists of the same but also includes 
              a predictor function, which predicts the representations of the teacher. The loss between the representations of teacher and student predictions are 
              minimized. When we use a randomly initialized, fixed network to produce teacher representations and train the student to predict those 
              representations, it avoids collapse, though the resulting representations are weak. Subsequently, if a student can improve upon a random fixed 
              teacher, then iteratively predicting better teacher representations yields increasingly stronger representations. As a result, from the teacher 
              representation, a potentially improved representation using the student network can be trained by predicting that teacher representation. Repeating 
              this process creates a sequence of progressively refined representations, using each student network as the next teacher. BYOL generalizes this 
              bootstrapping idea by updating the weights of only the student network and using a slow exponential moving average of the student network as the 
              teacher network.
            </p>
            <div class="paper-figure">
              <img src="images/byol.png" alt="BYOL architecture">
              <p class="caption">Figure: BYOL Architecture [3].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [4] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
          </div>
          <div class="paper-meta">
            <span>CVPR 2023</span>
            <span>Citations: 748 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Discriminative methods [2,3] may introduce strong biases that affect certain downstream tasks or pretraining tasks with different data distributions. 
              On the other hand, predictive methods like MAE [1] provide representations of lower semantic value relative to the discriminative methods and thus 
              require end-to-end fine-tuning to perform well on the downstream tasks. To address these limitations, I-JEPA aims to learn semantic representations 
              that do not require extensive finetuning on downstream tasks, and unlike the discriminative methods, do not incorporate extra prior knowledge 
              encoded through image transformations.
            </p>
            <p>
              I-JEPA learns abstract representations through the prediction of other regions of a masked image in the latent space. It consists of three main 
              components; context, target encoders and a predictor. The context encoder processes the visible context patches, while the predictor takes the 
              context encoder output and, conditioned on learnable mask tokens with added positional embeddings of the target block, predicts the target 
              representations at a specific location. It does this for various target blocks. The target representations are the outputs of the target-encoder. 
              The L2 loss is minimized between each block representation of the target encoder and the block predictions made by the predictor. The weights of 
              context encoder and predictor are updated using gradient descent while the weights of the target encoder are updated through the exponential moving 
              average (EMA) of the context encoder weights to avoid representation collapse. Similar to MAE [1], the masking strategy plays an important role to 
              learn meaningful representations. As a result, the target blocks need to be large enough to contain semantic information while the context block 
              should be spatially informative.  
            </p>
            <div class="paper-figure">
              <img src="images/i-jepa.png" alt="I-JEPA architecture">
              <p class="caption">Figure: I-JEPA Architecture [4].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [5] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language
          </div>
          <div class="paper-meta">
            <span>ICML 2022</span>
            <span>Citations: 1,205 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.mlr.press/v162/baevski22a/baevski22a.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Most prior research in self-supervised learning has focused on a specific modality (e.g. image, words, or speech), which leads to designing specific 
              architectures and learning biases that may not generalize to other modalities. Moreover, in computer vision, prior works consist of regressing/predicting 
              targets that contain information isolated to the current patch, such as visual tokens or pixels, which lack contextualized information (relevant features 
              from the entire image).  While some discriminative methods [3] use the latent target representation of the entire input, their aim is to learn 
              transformation-invariant features instead of structural information within a sample.
            </p>
            <p>
              Data2vec proposes regressing over the latent representations of the full input data using a masked view of the same input. It primarily consists of 
              a student and a teacher mode which share the same architecture. The teacher consists of taking as input a sequence of encoded tokens of the full 
              input and providing it to the ViT to get the latent representations. The student consists of transforming the same input into encoded tokens and 
              then randomly replacing tokens with a learnable mask embedding token, and feeding it to the ViT. The L2 loss between only the masked tokens of the 
              student and the corresponding positional tokens in the teacher representations are regressed. The weights of the student are updated using gradient 
              descent while the weights of the teacher are updated using the EMA of the student network. The teacher provides contextualized representations, 
              which contain richer information of the input data due to the self-attention mechanism of the transformer network. Moreover, they are normalized 
              and averaged outputs of last k-layers of the teacher network, since neural networks build features over multiple layers and providing different 
              types of features from multiple layers enriches the self-supervisory task.   
            </p>
            <div class="paper-figure">
              <img src="images/data2vec.png" alt="data2vec architecture">
              <p class="caption">Figure: Data2vec Architecture [5].</p>
            </div>
          </div>
        </article>
        
        <article class="paper-card">
          <div class="paper-title">
            [6] iBOT: Image Bert pre-training with Online Tokenizer
          </div>
          <div class="paper-meta">
            <span>ICLR 2022</span>
            <span>Citations: 1,310 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openreview.net/pdf?id=ydopy-e6Dg" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior works have focused primarily on the global views [2,3] for self-supervised learning, neglecting the internal structures of an image. 
              Empirically, the visual semantics tend to emerge progressively by bootstrapping the teacher representation that enforces similarity across 
              augmented views, but this process alone only captures global information and ignores the meaningful local patterns inside an image. As a result, 
              existing methods struggle to learn meaningful visual/patch-level semantics.
            </p>
            <p>
             iBOT combines learning local structural information using masked image modeling and global visual semantics using the cross-view alignment. In iBOT, 
              an image is sampled and two augmented views are created. Both views are passed through a student and a teacher network, but the view that's input 
              to the student undergoes random masking. The student encodes these masked (and unmasked) patches using a ViT, while the teacher encodes all the 
              patches. The student’s masked patch tokens are aligned to the corresponding teacher tokens using a cross-entropy (CE) loss. At the same time, iBOT 
              also aligns the [CLS] tokens across the two views, using CE loss, so that the model learns global visual semantics and transformation-invariant 
              features between the two correlated (augmented) views. The student is updated by gradient descent, and the teacher is updated as an EMA of the 
              student. Moreover, the same projection head is shared by the [CLS] token and each patch token to borrow the capability of semantic abstraction 
              acquired from the [CLS] token. 
            </p>
            <div class="paper-figure">
              <img src="images/ibot.png" alt="ibot architecture">
              <p class="caption">Figure: iBOT Architecture [6].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [7] Emerging properties in self-supervised vision transformers 
          </div>
          <div class="paper-meta">
            <span>ICCV 2021</span>
            <span>Citations: 9,350 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              ViTs have shown competitive performance against convolutional networks, however, they haven’t shown clear benefits over them in a supervised 
              learning setup. They require more computation, large amounts of training data to generalize and their features do not exhibit any unique properties. 
              Moreover, supervised learning does not leverage the context rich features present in an image and may use only a single concept from a predefined set 
              of a few thousand categories of objects.  Prior methods have proposed contrastive loss, adding predictor, or advanced normalization to avoid 
              representation collapse, however they add little benefit to stability of training and performance. Perhaps a simpler approach could be leveraged 
              while enjoying high performance. 
            </p>
            <p>
              DINO demonstrates that self-supervised ViT features contain rich semantics and perform very well with a k-nearest neighbor classifier on top, without 
              requiring finetuning. DINO begins by sampling an image and creating a set of augmented views that include two global views (>50% of the image) and 
              several local views (<50%). The global views are input to a teacher network, and the local views to a student network. Each network outputs a 
              K-dimensional feature that is normalized using a temperature-scaled softmax over the feature dimension. The latent representations of both networks 
              are aligned using a cross-entropy loss, encouraging “local-to-global’’ correspondences. The student is updated using SGD, while the teacher is 
              updated as an EMA of the student. DINO does not use batch-normalization and demonstrates that using smaller ViT patch sizes improves feature quality. 
              Representation collapse in this setting is not only about predicting constant vectors, but also includes uniform distributions (each feature ≈ 1/K) or 
              overly peaked distributions dominated by one dimension. Instead of using a predictor as in [3], DINO avoids collapse through centering and sharpening of 
              teacher representations. They are first centered by subtracting their running average (updated via EMA), which stabilizes training and enables 
              robustness across batch sizes. The centered outputs are then sharpened by dividing by a very small temperature and applying softmax. Centering alone 
              risks drifting toward uniform outputs, while sharpening alone risks producing peaked distributions, so applying both together achieves the best of both 
              worlds, avoiding collapse.
            </p>
            <div class="paper-figure">
              <img src="images/dino.png" alt="dino architecture">
              <p class="caption">Figure: DINO Architecture [7].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [8] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments 
          </div>
          <div class="paper-meta">
            <span>NeurIPS 2020</span>
            <span>Citations: 5,478 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Contrastive learning (CL) shows that aligning features from two augmented views while pushing apart negatives helps avoid collapse and yields good 
              representations. However, CL relies on random subsets of images to approximate-the-loss since it cannot use each and every pair to train, and also 
              needs very large batch sizes to generalize. Prior clustering-based approaches try to overcome this by assigning each image to a prototype and 
              training the model to predict these assignments for augmented views. Yet these methods follow a two-step offline approach where cluster assignments 
              must be recomputed over the entire dataset, which becomes impractical as dataset size grows. Another limitation is that, despite evidence that 
              multiple augmentations improve invariance, most methods still operate with only two augmented views.
            </p>
            <p>
              SwAV proposes to approximate-the-task by incorporating online clustering. For every image, several augmented views are created and passed through 
              an encoder to get feature vectors. Instead of directly comparing these features like in [2], each feature is assigned to one of several learnable 
              prototype vectors, which act like cluster centers. These assignments are called “codes.” Inspired by CL, SwAV then uses swapped alignment: the 
              feature from one view must align with the code produced by another view of the same image, and vice-versa. If the two views contain the same 
              underlying content, their codes should match, so this cross-view alignment encourages the encoder to learn consistent and meaningful representations. 
              To compute codes without storing the whole dataset, SwAV uses only the features within the current batch and applies a Sinkhorn normalization step 
              to ensure that prototypes are used evenly across the batch. This avoids trivial collapse where all features map to the same prototype. The method 
              also proposes using multi-crop augmentation, where one encoder receives two global views and the other encoder receives several local, low-resolution 
              crops of different sizes, to provide more diverse views and avoid biases in the features. Altogether, SwAV combines online clustering, balanced 
              assignments, and swapped alignment to learn representations without explicit negative samples or large memory banks.
            </p>
            <div class="paper-figure">
              <img src="images/swav.png" alt="swav architecture">
              <p class="caption">Figure: SwAV Architecture [8].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [9] Context Autoencoder for Self-supervised Representation Learning
          </div>
          <div class="paper-meta">
            <span>IJCV 2024</span>
            <span>Citations: 524 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://link.springer.com/content/pdf/10.1007/s11263-023-01852-4.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              The encoder of an SSL algorithm must learn highly informative and context-rich representations during pretraining to perform well on downstream 
              tasks. However, several prior works operating in the latent space do not explicitly separate representation learning from task completion, and ViTs 
              effectively mix encoder and decoder roles, which can limit representation capacity. For instance, in the masked image modeling component of the iBOT 
              algorithm [6], it does not separate the representation role and task completion role and uses both visible and masked patches as input, 
              simultaneously for the two roles. Similarly, in MAE [1], the decoder partially participates in representation learning because representations of 
              visible patches are also updated within the decoder, dispersing semantic learning across encoder-decoder. In view-invariance based methods [2,3], 
              cropping is critical for representation learning, yet most cropping strategies consistently include the image center. Since ImageNet-1K images 
              predominantly contain centered objects, this biases the model toward learning representations mainly from central regions, limiting coverage of the 
              full image. Consequently, objects away from the center may be treated as noise during contrastive learning, reducing the diversity of learned 
              representations.
            </p>
            <p>
              CAE proposes an encoder-regressor-decoder algorithm for representation learning. It decouples representation learning from solving the pretext task, 
              i.e., masked representation prediction and masked patch reconstruction. In the first step, an image is split into a sequence of patches, and only the 
              visible patches are passed through the encoder. In the second step, the latent representations of the visible patches, along with learnable masked 
              patch tokens, and their positional embeddings are passed into a regressor. The regressor consists of transformer blocks with cross-attention, where masked patch 
              tokens serve as queries and visible patch representations serve as keys and values, to predict the latent representations of the masked patches. In 
              the third step, the predicted masked latent representations are input to the decoder to reconstruct the discrete visual tokens of the masked patches, 
              i.e., tokens produced by a d-VAE trained on ImageNet-1K or the DALL-E tokenizer. The predicted masked latent representations are aligned with the masked 
              representations obtained from a target encoder using MSE loss. The gradients are stopped on the target encoder branch to prevent collapse. Simultaneously, the patch-level 
              reconstruction is optimized using cross-entropy loss over the discrete visual tokens. The encoder operates only on the visible patches, and CAE enables it to learn 
              context-rich representations of all the features present in an image.
            </p>
            <div class="paper-figure">
              <img src="images/cae.png" alt="cae architecture">
              <p class="caption">Figure: CAE Architecture [9].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [10] SimMIM: a Simple Framework for Masked Image Modeling
          </div>
          <div class="paper-meta">
            <span>CVPR 2022</span>
            <span>Citations: 1,981 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Masked signal modeling has shown great success with NLP in the past, and it has also widely been adapted in the computer vision area. When predicting 
              masked words, a self-supervised model learns features of high semantic value, however in vision, pixels carry low-level semantics, and the 
              reconstruction task becomes trivial if only a low percentage of pixels are masked in an image [1]. Recent methods have addressed this, but they use 
              specialized designs such as converting continuous signals into clusters, patch tokenization using an additional network, or block-wise masking 
              strategy to break short-range connections.
            </p>
            <p>
              SimMIM proposes a very simple framework that can be used for downstream tasks by finetuning the model once it has been pretrained. It consists of 
              three steps. (1) Randomly masking the patches within an image either using large patch sizes (e.g. 32) or high masking ratios (>80%), which 
              significantly reduces the chance of extrapolating neighboring pixels to recover the pixels. (2) The masked patches are replaced with learnable mask 
              token vectors, and then along with the visible patches, they are input to a ViT for obtaining the latent representations. (3) The latent 
              representations are decoded using only a single linear layer and the L1 loss is applied between only the original and predicted masked pixels. 
              SimMIMs main goal is to learn representations which can complement the following down-stream tasks by finetuning, rather than focussing on performing 
              well through linear probing.  
            </p>
            <div class="paper-figure">
              <img src="images/simmim.png" alt="simmim architecture">
              <p class="caption">Figure: SimMIM Architecture [10].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [11] Masked Siamese Networks for Label-Efficient Learning
          </div>
          <div class="paper-meta">
            <span>ECCV 2022</span>
            <span>Citations: 468 (as on date 12/09/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910442.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior works in masked image modeling [1] have shown great performance in self-supervised learning. However, optimizing the reconstruction loss 
              requires modeling low-level image details that are not necessary for the downstreams tasks that involve semantic abstraction. Moreover, they 
              require longer training hours and increased memory and computation to train and generalize.  
            </p>
            <p>
              MSN is a joint-embedding architecture. It learns representations through both view-transformations and masked image modeling. In the first step, 
              two instances of the same image are created and undergo random data augmentations. Let one be referred to as the anchor view and other as target view. 
              The anchor view then undergoes masking (either random or focal), after which it is passed to ViT encoder to obtain the [CLS] token output as its latent representation. 
              Similarly, the target view is also passed through a ViT to obtain its [CLS] token. A soft-distribution over a set of prototypes for both the anchor 
              and target views are obtained, similar to the online clustering approach [8]. The cross entropy loss is minimized, where the objective is to assign 
              the representation of the masked anchor view to the same prototypes as the representation of the unmasked target view. Additionally, a mean entropy 
              maximization regularizer is used to encourage the model to utilize the full set of prototypes. The weights of the anchor view are updated via 
              gradient descent while the weights of the target are updated through EMA of the anchor.
            </p>
            <div class="paper-figure">
              <img src="images/msn.png" alt="msn architecture">
              <p class="caption">Figure: MSN Architecture [11].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [12] Exploring Simple Siamese Representation Learning
          </div>
          <div class="paper-meta">
            <span>CVPR 2021</span>
            <span>Citations: 6,043 (as on date 12/21/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Learning through transformation invariance enables models to acquire context-rich representations that transfer well to downstream tasks. However, 
              many such methods rely on negative samples, large batch sizes [2], or momentum encoders [3] to prevent representation collapse. This motivates the 
              question of whether a simpler formulation, without these requirements, can still learn meaningful representations.
            </p>
            <p>
             SimSiam proposes a joint-embedding Siamese architecture that avoids both negative samples and momentum encoders. Two augmented views of the same 
              image are processed by a shared encoder and MLP projection head. The representation from one branch is passed through an additional MLP prediction 
              head to predict the representation of the other view. Training minimizes a negative cosine similarity loss, but gradients are propagated only 
              through the predictor branch, while a stop-gradient operation is applied to the target branch. The loss is symmetrized by swapping the roles of the 
              two views. Compared to BYOL [3], SimSiam removes the momentum encoder entirely and relies solely on architectural asymmetry and stop-gradient to 
              prevent collapse.
            </p>
            <div class="paper-figure">
              <img src="images/simsiam.png" alt="simsiam architecture">
              <p class="caption">Figure: SimSiam Architecture [12].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [13] BEiT: BERT PRE-TRAINING OF IMAGE TRANSFORMERS
          </div>
          <div class="paper-meta">
            <span>ICLR 2022</span>
            <span>Citations: 4,190 (as on date 12/22/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openreview.net/pdf?id=p-BhZSz59o4" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              BERT-like pretraining works very well in the language domain due to rich semantics. However, in vision, directly using a similar strategy in the 
              pixel space leads to two main issues: short-range dependencies and high-frequency details that are not necessary for semantic understanding. 
              Moreover, unlike language, pixels do not have a predefined vocabulary. As a result, a softmax classifier cannot be directly applied, since there 
              are no discrete classes or categories over which pixel values can be predicted. Perhaps, an intermediate representation that enables predicting a 
              vocabulary over the pixel space is required.
            </p>
            <p>
             BEiT proposes a masked image modeling approach that predicts discrete visual tokens instead of raw pixels. These discrete tokens are obtained by 
              first training a discrete variational autoencoder (dVAE). The dVAE encodes image patches into a sequence of discrete visual tokens and decodes them 
              back to reconstruct the original image patches. The dVAE training minimizes a reconstruction loss while learning a codebook that maps image patches 
              to discrete tokens. Once the dVAE is trained, BEiT performs pretraining by randomly masking image patches in a blockwise manner and replacing them 
              with learnable mask tokens. Both visible patch embeddings and mask tokens are passed through the BEiT encoder. The representations corresponding to 
              the masked tokens are then fed into a masked image modeling head to predict the corresponding discrete visual tokens produced by the dVAE. The 
              model is trained by minimizing a cross-entropy loss between the predicted token distribution and the target discrete visual tokens over the masked 
              patches.
            </p>
            <div class="paper-figure">
              <img src="images/beit.png" alt="beit architecture">
              <p class="caption">Figure: BEiT Architecture [13].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [14] Barlow Twins: Self-Supervised Learning via Redundancy Reduction
          </div>
          <div class="paper-meta">
            <span>ICML 2021</span>
            <span>Citations: 3,148 (as on date 12/26/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior self-supervised learning methods often rely on large batch sizes to prevent representation collapse or require carefully designed training 
              mechanisms, such as asymmetric architectures, momentum encoders, or stop-gradient operations. This motivates the question of whether a simpler 
              objective can be formulated to avoid collapse without such design choices. Inspired by Barlow’s redundancy-reduction principle in neuroscience, 
              which hypothesizes that sensory systems aim to recode highly redundant inputs into statistically independent components, Barlow Twins (BT) proposes 
              an objective that encourages the cross-correlation matrix between twin embeddings to approach the identity matrix.
            </p>
            <p>
              BT learns representations using a joint embedding architecture, where two distorted versions of each image in a batch are passed through identical 
              encoder and projection heads in parallel. Given the resulting batch embeddings from the two views, a cross-correlation matrix is computed using 
              cosine similarity between feature dimensions i and j across the batch. The diagonal elements enforce invariance by encouraging each feature to be 
              consistent across views, while the off-diagonal elements reduce redundancy by decorrelating different feature components. This objective is inspired 
              by the Information Bottleneck principle of reducing redundancy while preserving relevant information. It is also conceptually related to contrastive 
              learning; however, the BT loss operates across feature dimensions rather than across samples. Consequently, it benefits from very high-dimensional 
              embeddings in a manner similar to how contrastive methods benefit from large batch sizes.
            </p>
            <div class="paper-figure">
              <img src="images/bt.png" alt="bt architecture">
              <p class="caption">Figure: BT Architecture [14].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [15] Deconstructing Denoising Diffusion Models for Self-Supervised Learning
          </div>
          <div class="paper-meta">
            <span>ICLR 2025</span>
            <span>Citations: 136 (as on date 12/29/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openreview.net/pdf?id=9oMB6wnFYM" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Denoising Diffusion Models (DDMs) are at the forefront of image generation. Recently, the representation learning capabilities of off-the-shelf 
              DDMs have been explored in self-supervised learning (SSL); however, it remains unclear whether these representations arise from a denoising-driven 
              or from the diffusion-driven process. On the contrary, SSL methods based on predictive masking have demonstrated strong performance, outperforming 
              classical denoising autoencoders (DAEs) that inject additive gaussian noise in the pixel space. Masking-based methods such as MAE [1] have a slight 
              advantage where they explicitly define known versus unknown content within an image, whereas DAEs that predict clean images from additive noise lack 
              such explicit known/unknown supervision.
            </p>
            <p>
              Latent-DAE (l-DAE) is proposed by progressively deconstructing DDMs to identify the components critical for representation learning. The study shows 
              that SSL performance is not correlated to the generative quality of the model, and that directly using DDMs for SSL implicitly relies on components 
              that are illegitimate within the SSL paradigm. Moreover, several components designed to improve generation quality are unnecessary for learning good 
              representations. In l-DAE, an image is projected into a low-dimensional latent space using PCA, Gaussian noise is added in this latent space, and 
              the corrupted latent is mapped back to pixel space via inverse PCA. Then, an autoencoder is trained to predict the original clean image from this 
              noisy input. The “noise” consists of additive latent Gaussian noise and reconstruction error introduced by the PCA–inverse PCA projection. The 
              prediction error of the denoised image is calculated by projecting into PCA coordinates with a dimension-weighted MSE loss, where the first d 
              principal components (corresponding to the retained latent dimensions) are weighted by 1 and the remaining D−d components (with D denoting the full 
              image dimension) are down-weighted by 0.1. After training, only the encoder is used for downstream tasks. The key finding is that the latent 
              bottleneck (tokenizer) where noise is injected is the primary factor enabling strong representation learning. In this work, starting from a VQGAN 
              tokenizer and Diffusion Transformer (DiT), progressively the diffusion-specific components such as class conditioning, noise scheduling are 
              removed/replaced to approach a classical DAE. The tokenizer is replaced with a simple PCA–inverse PCA projection. While adding multi-level noise in 
              the latent space acts as a form of data augmentation and improves performance, even a single noise level yields good results with only a slight 
              degradation, indicating that representation learning is fundamentally a denoising-driven process and not diffusion-driven.
            </p>
            <div class="paper-figure">
              <img src="images/l-dae.png" alt="l-DAE architecture">
              <p class="caption">Figure: l-DAE Architecture [15].</p>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-title">
            [16] DINOv2: Learning Robust Visual Features without Supervision
          </div>
          <div class="paper-meta">
            <span>TMLR 2024</span>
            <span>Citations: 6,147 (as on date 01/02/2025)</span>
          </div>
          <div class="paper-link">
            <a href="https://openreview.net/pdf?id=a68SUt6zFt" target="_blank" rel="noopener noreferrer">
              Paper link
            </a>
          </div>
          <div class="paper-summary">
            <p>
              Prior works in self-supervised learning (SSL) have primarily focused on building algorithms in the context of small curated datasets, such as 
              ImageNet-1k. Conversely, only a few approaches have attempted SSL on large uncurated datasets, and the resulting representations were often poor. 
              This is largely due to the loss of data quality and effective diversity control, both of which are essential components for learning strong 
              representations.
            </p>
            <p>
              DINOv2 addresses this by using an automatic data preprocessing pipeline that combines curated and uncurated datasets, while also integrating 
              several existing SSL methods into a more robust and scalable framework in which ViT encoders of different sizes are trained. In the preprocessing 
              step, curated data are first collected and uncurated data are gathered from a large repository. Deduplication is applied to the uncurated data 
              using a copy-detection pipeline to remove highly similar images and reduce redundancy. Images from both curated and uncurated datasets are then 
              embedded into a latent space using a pretrained ViT-H/14 SSL encoder, and cosine similarity is computed between their embeddings. Using this 
              similarity, K-nearest neighbors from the uncurated set are queried for each curated image; if many neighbors exist, the nearest neighbor count is 
              capped at N=4 to maintain diversity, while fewer samples are drawn for smaller clusters. This process results in the LVD-142M dataset. DINOv2 
              training combines DINO and iBOT losses, where a cross-entropy loss is applied on global crops using the CLS + projection head outputs of the 
              student and teacher networks, and a patch-level cross-entropy loss is computed by masking patches in the student input and matching them to 
              corresponding teacher outputs. Unlike iBOT, DINOv2 does not share projection heads between global and patch-level objectives, and replaces the 
              DINO teacher softmax-centering with Sinkhorn-Knopp batch normalization from SwAV, while applying softmax to the student outputs. Additionally, a 
              KoLeo regularizer is introduced to prevent representation collapse by encouraging feature embeddings within a batch to be uniformly spread on the 
              unit hypersphere. DINOv2 also progressively increases image resolution toward the end of training to better capture fine-grained details. To enable 
              large-scale training, several efficiency improvements are employed to reduce memory and compute costs. Finally, instead of training smaller ViT 
              models from scratch, DINOv2 uses knowledge distillation by training a large ViT-g model and distilling it into smaller models, using a frozen ViT-g 
              teacher, evaluating the EMA of the student as the final model, and applying only the iBOT loss on the two global crops.
            </p>
            <div class="paper-figure">
              <img src="images/dinov2.png" alt="DINOv2 Data Preprocessing Pipeline">
              <p class="caption">Figure: DINOv2 Data Preprocessing Pipeline [16].</p>
            </div>
          </div>
        </article>
      </div>
    </section>

    
    <!-- TABLE SECTION -->
    <h2>Algorithm vs Feature Comparison</h2>
    <section>
      <h3>Input</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Random Masking</th>
              <th>Additive Gaussian Noise</th>
              <th>Data Augmentation</th>
              <th>Multiple Augmentations</th>
              <th>Negative-pair samples</th>
              <th>Local views</th>
              <th>Global views</th>
              <th>Data Preprocessing</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BEiT [13]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Output</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>[CLS] embedding + projection head</th>
              <th>[CLS] embedding</th>
              <th>Latent patch embedding + projection head</th>
              <th>Latent patch embedding</th>
              <th>Patch/Pixels</th>
              <th>Patch Tokens (dVAE or DALL-E)</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BEiT [13]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Architecture</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Predictive method</th>
              <th>Alignment method</th>
              <th>View-invariance method</th>
              <th>Asymmetric Architecture</th>
              <th>Bootstrapping</th>
              <th>Normalization</th>
              <th>Clustering</th>
              <th>Matrix Computation</th>
              <th>Last-k layer averaged target</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Loss</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>MAE/L1</th>
              <th>MSE/L2</th>
              <th>weighted-MSE</th>
              <th>CE</th>
              <th>Contrastive Learning</th>
              <th>Cosine Similarity</th>
              <th>Patch-level</th>
              <th>Representation-level</th>
              <th>Any extra regularizer</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BEiT [13]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Inductive biases</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Prior knowledge</th>
              <th>Learns representations through view-transformations</th>
              <th>Learns representations through structural information</th>
              <th>Full image contextualization</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BEiT [13]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
              <td class="check"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <section>
      <h3>Training requirements</h3>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Algorithm</th>
              <th>Large batch sizes increase performance</th>
              <th>Large feature dimensions increase performance</th>
              <th>Long training (> 1500 epochs)</th>
            </tr>
          </thead>
          <tbody>
            <!-- Example rows, change ✓/blank as needed -->
            <tr>
              <td>MAE [1]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimCLR [2]</td>
              <td class="check"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>BYOL [3]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>I-JEPA [4]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>Data2vec [5]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>iBOT [6]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>DINO [7]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SwAV [8]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>CAE [9]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>SimMIM [10]</td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>MSN [11]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>SimSiam [12]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BEiT [13]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>BT [14]</td>
              <td class="empty"></td>
              <td class="check"></td>
              <td class="empty"></td>
            </tr>
            <tr>
              <td>l-DAE [15]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="check"></td>
            </tr>
            <tr>
              <td>DINOv2 [16]</td>
              <td class="empty"></td>
              <td class="empty"></td>
              <td class="empty"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    
    

    <!-- <footer>
      
    </footer> -->
  </div>
</body>
</html>
